{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "âœ‹BREAKOUT ROOM #1:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: Basic RAG Chain\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "\n",
        "âœ‹BREAKOUT ROOM #2:\n",
        "- Task 1: Parent Document Retriever\n",
        "- Task 2: Ensemble Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSjB1O6-Y0J",
        "outputId": "9cc0c072-1117-4863-8010-ee37e8e33a3d"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "a5794372-be42-46ee-cf7d-4e5628e97e9a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_NpPwk1YAgl"
      },
      "source": [
        "## Task 2: Basic RAG Chain\n",
        "\n",
        "Now we'll set up our basic RAG chain, first up we need a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUWXhsNVYLTA"
      },
      "source": [
        "### OpenAI Model\n",
        "\n",
        "\n",
        "We'll use OpenAI's `gpt-3.5-turbo` model to ensure we can use a stronger model for decent evaluation later!\n",
        "\n",
        "Notice that we can tag our resources - this will help us be able to keep track of which resources were used where later on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "CSgK6jgw_tI3"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", tags=[\"base_llm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiagvgVDYTPn"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ntIqnv4cA5gR"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDO0XJqbYabb"
      },
      "source": [
        "### SiteMap Loader\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAS3QBQSARiw",
        "outputId": "c10b7cc4-e5c7-4dd8-c689-59d5c356c4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lxml in /opt/miniconda3/lib/python3.12/site-packages (5.2.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching pages: 100%|##########| 219/219 [00:04<00:00, 52.52it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install lxml\n",
        "\n",
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_s_x87H0BYmn",
        "outputId": "2c74e338-8ba6-432f-c2da-641d5d55336e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://blog.langchain.dev/langgraph-cloud/'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].metadata[\"source\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F79PdFcaYfBL"
      },
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "\n",
        "We're going to use a relatively naive text splitting strategy today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "NmCdYTTTA4du"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "split_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 20\n",
        ").split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLA5-LNBBVM-",
        "outputId": "f5a05c71-c382-42f4-e6a5-58f3b61d66f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUsEc07iYnwj"
      },
      "source": [
        "### Embeddings\n",
        "\n",
        "We'll be leveraging OpenAI's [text-embedding-3-small](https://openai.com/index/new-embedding-models-and-api-updates/) today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "QVhMN0aaBrsM"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "base_embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLoO_2MaY0TS"
      },
      "source": [
        "### Qdrant VectorStore Retriever\n",
        "\n",
        "Now we can use a Qdrant VectorStore to embed and store our documents and then convert it to a retriever so it can be used in our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "nBTK9kSFBWM1"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import Qdrant\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    split_documents,\n",
        "    base_embeddings_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"langchainblogs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ZpwDxlniCJRu"
      },
      "outputs": [],
      "source": [
        "base_retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2GPhHPAY5yG"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "All we have left is a prompt template, which we'll create here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "YAU74penCNmR"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmT5VyLmZAAK"
      },
      "source": [
        "### LCEL Chain\n",
        "\n",
        "Now that we have:\n",
        "\n",
        "- Embeddings Model\n",
        "- Generation Model\n",
        "- Retriever\n",
        "- Prompt\n",
        "\n",
        "We're ready to build our LCEL chain!\n",
        "\n",
        "Keep in mind that we're returning our source documents with our queries - while this isn't necessary, it's a great thing to get into the habit of doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "pqVAsUc_Cp-7"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "base_rag_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fNjMoS-ZVo5"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6Dq9rCScDfBE",
        "outputId": "4bcebeb0-37ae-4fb8-9dae-ceba9cc1dc54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"A good way to evaluate agents is by testing their capabilities in tasks such as planning, task decomposition, function calling, and the ability to override pre-trained biases when needed. Additionally, assessing an agent's performance across different tasks and considering its ability to generalize reliably for various workflows where reasoning is required can be helpful in evaluating agents effectively.\""
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is a good way to evaluate agents?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "587380f0-7395-4608-aa63-35d117dbd162"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3eoqBtBQERXP",
        "outputId": "727abc25-3510-49b7-9671-98406e672294"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangSmith is a unified platform for debugging, testing, evaluating, and monitoring LLM applications. It allows users to easily debug, monitor, test, evaluate, and share their LLM applications.'"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Include a screenshot of your trace and explain what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMzWpDK369i2"
      },
      "source": [
        "![image](https://i.imgur.com/WQtlxc5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Create Testing Dataset\n",
        "\n",
        "Now we can create a dataset using some user defined questions, and providing the retrieved context as a \"ground truth\" context.\n",
        "\n",
        "> NOTE: There are many different ways you can approach this specific task - generating ground truth answers with AI, using human experts to generate golden datasets, and more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCKCAhASkXu0"
      },
      "source": [
        "### Synthetic Data Generation (SDG)\n",
        "\n",
        "In order to full test our RAG chain, and the various modifications we'll be using in the following notebook, we'll need to create a small synthetic dataset that is relevant to our task!\n",
        "\n",
        "Let's start by generating a series of questions - which begins with a simple model definition!\n",
        "\n",
        "> NOTE: We're going to be using a purposefully simplified datagen pipeline as an example today - but you could leverage the RAGAS SDG pipeline just as easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "-dgAkRzDlEgH"
      },
      "outputs": [],
      "source": [
        "question_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-HC4C8lWIS"
      },
      "source": [
        "Next up, we'll create some novel chunks from our source data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "2Sz7rw0-lhf8"
      },
      "outputs": [],
      "source": [
        "sdg_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 20\n",
        ").split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pep-eqUbllrv"
      },
      "source": [
        "Now, let's ask some questions that could be answered from the provided chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "e75gwn4Tlt3w"
      },
      "outputs": [],
      "source": [
        "question_prompt_template = \"\"\"\\\n",
        "You are a University Professor creating questions for an exam. You must create a question for a given piece of context.\n",
        "\n",
        "The question must be answerable only using the provided context.\n",
        "\n",
        "Avoid creating questions that are ambiguous or vague. They should be specifically related to the context.\n",
        "\n",
        "Your output must only be the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "question_prompt = ChatPromptTemplate.from_template(question_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "Pcb_R-G_odCl"
      },
      "outputs": [],
      "source": [
        "question_chain = question_prompt | question_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5YbGi7umtOB"
      },
      "source": [
        "Now we can loop through a subset of our context chunks and create question/context pairs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot0HzAGBms90",
        "outputId": "0417f0df-ea17-4da0-dd3b-a6336b154c4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:15<00:00,  1.45it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "question_context_pairs = []\n",
        "\n",
        "for idx in tqdm(range(0, len(sdg_documents), 40)):\n",
        "  question = question_chain.invoke({\"context\" : sdg_documents[idx].page_content})\n",
        "  question_context_pairs.append({\"question\" : question.content, \"context\" : sdg_documents[idx].page_content, \"idx\" : idx})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzhg3AJlmfW-",
        "outputId": "9c32bad2-f2db-471d-e526-ad430e427292"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What is the name of the new infrastructure for running agents at scale that is available in beta, mentioned in the provided context?',\n",
              " 'context': 'Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\nOur new infrastructure for running agents at scale, LangGraph Cloud, is available in beta. We also have a new stable release of LangGraph.\\n\\n6 min read\\nJun 27, 2024',\n",
              " 'idx': 0}"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_context_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34zGDKtD9I0A"
      },
      "source": [
        "We'll repeat this process for answers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Ws624hGapJ92"
      },
      "outputs": [],
      "source": [
        "answer_prompt_template = \"\"\"\\\n",
        "You are a University Professor creating an exam. You must create a answer for a given piece of context and question.\n",
        "\n",
        "The answer must only rely on the provided context.\n",
        "\n",
        "Your output must only be the answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_template(answer_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "AEnETuSEqf2R"
      },
      "outputs": [],
      "source": [
        "answer_chain = answer_prompt | question_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89faPBcPqnfT",
        "outputId": "8e9bd52a-b400-44da-d228-34d9234c324e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:14<00:00,  1.47it/s]\n"
          ]
        }
      ],
      "source": [
        "for question_context_pair in tqdm(question_context_pairs):\n",
        "  question_context_pair[\"answer\"] = answer_chain.invoke({\"question\" : question_context_pair[\"question\"], \"context\" : question_context_pair[\"context\"]}).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usB9PkJWq_0D",
        "outputId": "4cc1d45b-c389-4d18-a7cf-37e8efbdcf00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What is the name of the new infrastructure for running agents at scale that is available in beta, mentioned in the provided context?',\n",
              " 'context': 'Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\nOur new infrastructure for running agents at scale, LangGraph Cloud, is available in beta. We also have a new stable release of LangGraph.\\n\\n6 min read\\nJun 27, 2024',\n",
              " 'idx': 0,\n",
              " 'answer': 'LangGraph v0.1 & LangGraph Cloud.'}"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_context_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-v2\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in question_context_pairs:\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "65d1b263094a4c6abdaf8688285c41cc",
            "087e4c759a35424385156c76e3780144",
            "18ffc7ae91d1408c8747f8d0a972f3a8",
            "8cbac09f7ba540bd8f79dde47e49fe5f",
            "b5e2be01c1834c78b7ce0cbe1f7a9d09",
            "ef2e833ac73548ee837f906d4b004a3d",
            "2c984bd9861c4ec0b27f0c3a2020dede",
            "c9927b2052be4016b0feb88ab583d32e",
            "6fbddf6663954ff48ee0cb0b4a202acb",
            "e6035d68dd9e4c6598ac5eae2d0deac1",
            "b6b1f2be05a34e118dc55869149b60ac"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "528f629a-abf9-4f9b-9af4-3756aa40cc50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Base RAG Evaluation-2507d718' at:\n",
            "https://smith.langchain.com/o/3e2d3fa4-8d65-5328-ab1f-a089d874c8f0/datasets/5f75ca1f-b2ae-4821-9cf0-9ea7ca5ec2ae/compare?selectedSessions=11677221-de95-46df-a009-0e66a93f2cff\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6c2e41549a2420c90ed44d385ab767b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1fec6ab4-82a6-4fab-9a14-29bb7f5d1aaf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9939, Requested 243. Please try again in 1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9939, Requested 243. Please try again in 1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 22d4a208-a2e2-44dc-b12d-1f16cf7561b6: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 313. Please try again in 1.428s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 313. Please try again in 1.428s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 25d0326c-a615-4218-b8a6-445be6bfb531: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9921, Requested 335. Please try again in 1.536s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9921, Requested 335. Please try again in 1.536s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d8579989-f8cd-41d7-b36d-e6218111d18d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9950, Requested 306. Please try again in 1.536s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9950, Requested 306. Please try again in 1.536s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d7bf55a-6294-4b17-8c35-494218a5029c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9874, Requested 367. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9874, Requested 367. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a43c1242-2019-4345-bc9e-a738644b043e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9939, Requested 303. Please try again in 1.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9939, Requested 303. Please try again in 1.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3f9cc68c-ec9f-4cff-bbcb-3e8b1cef73c8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9803, Requested 242. Please try again in 270ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9803, Requested 242. Please try again in 270ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 10299243-e0f2-440a-b95c-a5933b098727: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9802, Requested 245. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9802, Requested 245. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d1baae10-666a-4fdc-8f9a-112e7354377d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9766, Requested 279. Please try again in 270ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9766, Requested 279. Please try again in 270ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2daf1a08-6cfe-4abe-a739-ba5f8c726e40: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9961, Requested 261. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9961, Requested 261. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d8579989-f8cd-41d7-b36d-e6218111d18d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9949, Requested 282. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9949, Requested 282. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 25d0326c-a615-4218-b8a6-445be6bfb531: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9931, Requested 296. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9931, Requested 296. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d7bf55a-6294-4b17-8c35-494218a5029c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9923, Requested 308. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9923, Requested 308. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a43c1242-2019-4345-bc9e-a738644b043e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 302. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 302. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 22d4a208-a2e2-44dc-b12d-1f16cf7561b6: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9917, Requested 310. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9917, Requested 310. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4391ef6-d274-440a-8210-55f65f075b2e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9955, Requested 509. Please try again in 2.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9955, Requested 509. Please try again in 2.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c5eb99fb-b3a9-49a9-b848-89cb2fcf8f7e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9897, Requested 331. Please try again in 1.368s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9897, Requested 331. Please try again in 1.368s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 27d65899-adde-408a-8958-fd7a238c707a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9869, Requested 361. Please try again in 1.38s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9869, Requested 361. Please try again in 1.38s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3f9cc68c-ec9f-4cff-bbcb-3e8b1cef73c8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9974, Requested 267. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9974, Requested 267. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d1baae10-666a-4fdc-8f9a-112e7354377d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9970, Requested 272. Please try again in 1.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9970, Requested 272. Please try again in 1.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8af8f13b-034d-4c42-a1df-dd3b08f9d48c: ValueError(\"Invalid output: The assistant's response cannot be evaluated for accuracy as the ground truth does not provide a reference answer. The assistant's response seems to be in line with the question asked, but without a reference answer, it's impossible to determine its accuracy. Rating: [[N/A]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 128, in _call\n",
            "    return self.create_outputs(response)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 285, in create_outputs\n",
            "    self.output_key: self.output_parser.parse_result(generation),\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/output_parsers/base.py\", line 221, in parse_result\n",
            "    return self.parse(result[0].text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 134, in parse\n",
            "    raise ValueError(\n",
            "ValueError: Invalid output: The assistant's response cannot be evaluated for accuracy as the ground truth does not provide a reference answer. The assistant's response seems to be in line with the question asked, but without a reference answer, it's impossible to determine its accuracy. Rating: [[N/A]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c5eb99fb-b3a9-49a9-b848-89cb2fcf8f7e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9963, Requested 289. Please try again in 1.512s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9963, Requested 289. Please try again in 1.512s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2daf1a08-6cfe-4abe-a739-ba5f8c726e40: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9974, Requested 283. Please try again in 1.542s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9974, Requested 283. Please try again in 1.542s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a43c1242-2019-4345-bc9e-a738644b043e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9937, Requested 318. Please try again in 1.53s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9937, Requested 318. Please try again in 1.53s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d8579989-f8cd-41d7-b36d-e6218111d18d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9933, Requested 322. Please try again in 1.53s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9933, Requested 322. Please try again in 1.53s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 22d4a208-a2e2-44dc-b12d-1f16cf7561b6: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 328. Please try again in 1.542s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 328. Please try again in 1.542s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 25d0326c-a615-4218-b8a6-445be6bfb531: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9909, Requested 350. Please try again in 1.554s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9909, Requested 350. Please try again in 1.554s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4391ef6-d274-440a-8210-55f65f075b2e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9888, Requested 369. Please try again in 1.542s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9888, Requested 369. Please try again in 1.542s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d7bf55a-6294-4b17-8c35-494218a5029c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9847, Requested 382. Please try again in 1.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9847, Requested 382. Please try again in 1.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 27d65899-adde-408a-8958-fd7a238c707a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9919, Requested 339. Please try again in 1.548s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9919, Requested 339. Please try again in 1.548s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d1baae10-666a-4fdc-8f9a-112e7354377d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9799, Requested 294. Please try again in 558ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9799, Requested 294. Please try again in 558ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 924d8bda-393c-49e2-a71b-43d19d49fdc3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9771, Requested 322. Please try again in 558ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9771, Requested 322. Please try again in 558ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f347c96a-9fc1-4ef1-9b93-73d41b20b730: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9907, Requested 338. Please try again in 1.47s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9907, Requested 338. Please try again in 1.47s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c5eb99fb-b3a9-49a9-b848-89cb2fcf8f7e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9899, Requested 346. Please try again in 1.47s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9899, Requested 346. Please try again in 1.47s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b8742aa6-1f7f-41ee-a7ec-f8b919842e1e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9888, Requested 352. Please try again in 1.44s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9888, Requested 352. Please try again in 1.44s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 27d65899-adde-408a-8958-fd7a238c707a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9869, Requested 376. Please try again in 1.47s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9869, Requested 376. Please try again in 1.47s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e81d78e2-c4de-4ec4-9949-be8479fd5f67: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9855, Requested 386. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9855, Requested 386. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 89cec5f6-a62e-42f6-b81c-56beccd7ae1c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9777, Requested 844. Please try again in 3.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9777, Requested 844. Please try again in 3.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 206cb4ec-f920-45d5-a44b-83492f1ffef1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9911, Requested 395. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9911, Requested 395. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 533fb81a-10fa-40c6-8e5d-15186625fbe4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9879, Requested 429. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9879, Requested 429. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f1122ed1-0281-40fa-bc0c-41f0ccb2147c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9858, Requested 450. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9858, Requested 450. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b8742aa6-1f7f-41ee-a7ec-f8b919842e1e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9801, Requested 275. Please try again in 456ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9801, Requested 275. Please try again in 456ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f347c96a-9fc1-4ef1-9b93-73d41b20b730: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9812, Requested 265. Please try again in 462ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9812, Requested 265. Please try again in 462ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4391ef6-d274-440a-8210-55f65f075b2e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9792, Requested 524. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9792, Requested 524. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e81d78e2-c4de-4ec4-9949-be8479fd5f67: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9772, Requested 304. Please try again in 456ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9772, Requested 304. Please try again in 456ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b8742aa6-1f7f-41ee-a7ec-f8b919842e1e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9972, Requested 291. Please try again in 1.578s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9972, Requested 291. Please try again in 1.578s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 533fb81a-10fa-40c6-8e5d-15186625fbe4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9974, Requested 288. Please try again in 1.572s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9974, Requested 288. Please try again in 1.572s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 206cb4ec-f920-45d5-a44b-83492f1ffef1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9942, Requested 324. Please try again in 1.596s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9942, Requested 324. Please try again in 1.596s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 89cec5f6-a62e-42f6-b81c-56beccd7ae1c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9872, Requested 392. Please try again in 1.584s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9872, Requested 392. Please try again in 1.584s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f1122ed1-0281-40fa-bc0c-41f0ccb2147c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9952, Requested 309. Please try again in 1.566s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9952, Requested 309. Please try again in 1.566s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e81d78e2-c4de-4ec4-9949-be8479fd5f67: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9964, Requested 325. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9964, Requested 325. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 206cb4ec-f920-45d5-a44b-83492f1ffef1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9759, Requested 334. Please try again in 558ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9759, Requested 334. Please try again in 558ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 533fb81a-10fa-40c6-8e5d-15186625fbe4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9987, Requested 367. Please try again in 2.124s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9987, Requested 367. Please try again in 2.124s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f1122ed1-0281-40fa-bc0c-41f0ccb2147c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9966, Requested 389. Please try again in 2.13s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9966, Requested 389. Please try again in 2.13s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e81d78e2-c4de-4ec4-9949-be8479fd5f67: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9940, Requested 301. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9940, Requested 301. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f347c96a-9fc1-4ef1-9b93-73d41b20b730: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9783, Requested 263. Please try again in 276ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9783, Requested 263. Please try again in 276ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f1122ed1-0281-40fa-bc0c-41f0ccb2147c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9963, Requested 306. Please try again in 1.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9963, Requested 306. Please try again in 1.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 206cb4ec-f920-45d5-a44b-83492f1ffef1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9948, Requested 322. Please try again in 1.62s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9948, Requested 322. Please try again in 1.62s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e81d78e2-c4de-4ec4-9949-be8479fd5f67: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9936, Requested 340. Please try again in 1.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9936, Requested 340. Please try again in 1.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f347c96a-9fc1-4ef1-9b93-73d41b20b730: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 292. Please try again in 1.326s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 292. Please try again in 1.326s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b8742aa6-1f7f-41ee-a7ec-f8b919842e1e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9938, Requested 306. Please try again in 1.464s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9938, Requested 306. Please try again in 1.464s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 89cec5f6-a62e-42f6-b81c-56beccd7ae1c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9732, Requested 859. Please try again in 3.546s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9732, Requested 859. Please try again in 3.546s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f1122ed1-0281-40fa-bc0c-41f0ccb2147c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9932, Requested 404. Please try again in 2.015s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9932, Requested 404. Please try again in 2.015s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 533fb81a-10fa-40c6-8e5d-15186625fbe4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9646, Requested 382. Please try again in 168ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9646, Requested 382. Please try again in 168ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", prepare_data=prepare_data_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "unlabeled_coherence_evaluator = LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"coherence\"}, prepare_data=prepare_data_noref)\n",
        "labeled_relevance_evaluator = LangChainStringEvaluator(\"labeled_criteria\", config={ \"criteria\": \"relevance\"}, prepare_data=prepare_data_ref)\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    base_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwhBxlxYAdno"
      },
      "source": [
        "## Testing Other Retrievers\n",
        "\n",
        "Now we can test our how changing our Retriever impacts our LangSmith evaluation!\n",
        "\n",
        "We'll build this simple qa_chain factory to create standardized qa_chains where the only different component will be the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "qnfy4VNkzZi2"
      },
      "outputs": [],
      "source": [
        "def create_qa_chain(retriever):\n",
        "  primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "  created_qa_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | RunnablePassthrough.assign(\n",
        "        context=itemgetter(\"context\")\n",
        "      )\n",
        "    | {\n",
        "         \"response\": base_rag_prompt | primary_qa_llm,\n",
        "         \"context\": itemgetter(\"context\"),\n",
        "      }\n",
        "  )\n",
        "  return created_qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOPp4Xq7AvEx"
      },
      "source": [
        "### Task 1: Parent Document Retriever\n",
        "\n",
        "One of the easier ways we can imagine improving a retriever is to embed our documents into small chunks, and then retrieve a significant amount of additional context that \"surrounds\" the found context.\n",
        "\n",
        "You can read more about this method [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever)!\n",
        "\n",
        "The basic outline of this retrieval method is as follows:\n",
        "\n",
        "1. Obtain User Question\n",
        "2. Retrieve child documents using Dense Vector Retrieval\n",
        "3. Merge the child documents based on their parents. If they have the same parents - they become merged.\n",
        "4. Replace the child documents with their respective parent documents from an in-memory-store.\n",
        "5. Use the parent documents to augment generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "67I6QJAJ0Un7"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=\"split_parents\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vectorstore = Qdrant(client, collection_name=\"split_parents\", embeddings=base_embeddings_model)\n",
        "\n",
        "store = InMemoryStore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "zfk5RYUt00Pw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=RecursiveCharacterTextSplitter(chunk_size=400),\n",
        "    parent_splitter=RecursiveCharacterTextSplitter(chunk_size=2000),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "68c1t4o104AK"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTH0MDolBndm"
      },
      "source": [
        "Let's create, test, and then evaluate our new chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "KMjLfqOC09Iw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Rv8bAHPN1H4P",
        "outputId": "46487575-438d-4c12-d306-7a4341d7ed83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in Large Language Model (LLM) application development. It involves connecting LLMs to external data sources to retrieve relevant documents based on a user query and generate an answer grounded in the retrieved context.'"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6H7vHCt2HJi"
      },
      "source": [
        "#### Evaluating the Parent Document Retrieval Pipeline\n",
        "\n",
        "Now that we've created a new retriever - let's try evaluating it on the same dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "142e5756a6c840aeb25b733ece015ed2",
            "4777a17d7d9c444a8d44e070faef0ad0",
            "dad645f3259c41b5a50e0e94906ca3e3",
            "c67428191d074e4d80cb0f28fb65cd41",
            "8db8082c5ab741b5b4fd78027e65135d",
            "2ab8452345f24b46b172515ddc77fdb0",
            "d5ad942ac5ed41c6a581b4022f177114",
            "27e2bff47a084ed6bc7d96ac09af9ebc",
            "d097ee02256d40d5b395d37cd0face05",
            "13c75ca5ee9d42eb8476a307f30b7528",
            "e8143f8ceee6468c98433734ff59b835"
          ]
        },
        "id": "Z-0WFCtx2N4n",
        "outputId": "341b1af3-cd74-46a9-d9d1-5ab2190ec96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Parent Document Retrieval RAG Evaluation-39526c40' at:\n",
            "https://smith.langchain.com/o/3e2d3fa4-8d65-5328-ab1f-a089d874c8f0/datasets/9eff5d3e-6c80-4567-811b-bd50d532ff2d/compare?selectedSessions=dac6a3f5-a605-4343-8659-d1c4cb63057c\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fde2261e7bc42c3b53deefe640cbdea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 73362e6f-8192-40cc-b7a1-9a22890cb1b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9914, Requested 382. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9914, Requested 382. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 630036bf-7491-4c21-973d-cd27fde35b96: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9921, Requested 403. Please try again in 1.944s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9921, Requested 403. Please try again in 1.944s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 313884b4-3281-4593-9af9-75e645baeb29: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9915, Requested 411. Please try again in 1.955s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9915, Requested 411. Please try again in 1.955s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 73362e6f-8192-40cc-b7a1-9a22890cb1b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9758, Requested 389. Please try again in 882ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9758, Requested 389. Please try again in 882ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 630036bf-7491-4c21-973d-cd27fde35b96: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 392. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 392. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 313884b4-3281-4593-9af9-75e645baeb29: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9920, Requested 401. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9920, Requested 401. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7439cbb0-ab32-4ad7-a304-7a66244f307f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9811, Requested 325. Please try again in 816ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9811, Requested 325. Please try again in 816ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 73362e6f-8192-40cc-b7a1-9a22890cb1b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9742, Requested 379. Please try again in 726ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9742, Requested 379. Please try again in 726ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8888a90e-cc0c-4de6-b58d-1dcfcfa66dbb: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9927, Requested 321. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9927, Requested 321. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a9e94a43-aab0-442d-939c-80e3664631f4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 325. Please try again in 1.5s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 325. Please try again in 1.5s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 044f8611-3f2d-43d6-8db2-f0c83575fcf7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9901, Requested 347. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9901, Requested 347. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f95d93db-ee9f-4b84-ad71-65ab0ca8034a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9901, Requested 347. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9901, Requested 347. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7439cbb0-ab32-4ad7-a304-7a66244f307f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9917, Requested 333. Please try again in 1.5s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9917, Requested 333. Please try again in 1.5s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2ef8cdf3-89dd-4a1c-83f9-d6bc1e6837b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9877, Requested 375. Please try again in 1.512s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9877, Requested 375. Please try again in 1.512s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 630036bf-7491-4c21-973d-cd27fde35b96: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9848, Requested 401. Please try again in 1.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9848, Requested 401. Please try again in 1.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 041acdd3-974c-4476-9696-cde7e10fb2e9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9763, Requested 354. Please try again in 702ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9763, Requested 354. Please try again in 702ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d652e7c-88f2-4ffc-970d-4904c645ae5b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9904, Requested 410. Please try again in 1.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9904, Requested 410. Please try again in 1.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 313884b4-3281-4593-9af9-75e645baeb29: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9905, Requested 408. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9905, Requested 408. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a9e94a43-aab0-442d-939c-80e3664631f4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9849, Requested 334. Please try again in 1.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9849, Requested 334. Please try again in 1.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7439cbb0-ab32-4ad7-a304-7a66244f307f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9842, Requested 340. Please try again in 1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9842, Requested 340. Please try again in 1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 044f8611-3f2d-43d6-8db2-f0c83575fcf7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9838, Requested 343. Please try again in 1.086s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9838, Requested 343. Please try again in 1.086s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8888a90e-cc0c-4de6-b58d-1dcfcfa66dbb: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9850, Requested 330. Please try again in 1.08s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9850, Requested 330. Please try again in 1.08s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f95d93db-ee9f-4b84-ad71-65ab0ca8034a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9839, Requested 346. Please try again in 1.11s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9839, Requested 346. Please try again in 1.11s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3c8161e2-dcfb-4500-93f5-74d0a7507906: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9845, Requested 339. Please try again in 1.104s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9845, Requested 339. Please try again in 1.104s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2ef8cdf3-89dd-4a1c-83f9-d6bc1e6837b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9808, Requested 375. Please try again in 1.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9808, Requested 375. Please try again in 1.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 73362e6f-8192-40cc-b7a1-9a22890cb1b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9895, Requested 404. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9895, Requested 404. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 630036bf-7491-4c21-973d-cd27fde35b96: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9890, Requested 407. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9890, Requested 407. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cda8e74c-bdba-48de-bc56-4606d00c6c38: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9829, Requested 469. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9829, Requested 469. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a9e94a43-aab0-442d-939c-80e3664631f4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9980, Requested 341. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9980, Requested 341. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8888a90e-cc0c-4de6-b58d-1dcfcfa66dbb: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9984, Requested 336. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9984, Requested 336. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 041acdd3-974c-4476-9696-cde7e10fb2e9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9967, Requested 355. Please try again in 1.932s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9967, Requested 355. Please try again in 1.932s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 044f8611-3f2d-43d6-8db2-f0c83575fcf7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9958, Requested 362. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9958, Requested 362. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3c8161e2-dcfb-4500-93f5-74d0a7507906: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9977, Requested 340. Please try again in 1.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9977, Requested 340. Please try again in 1.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f95d93db-ee9f-4b84-ad71-65ab0ca8034a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9956, Requested 363. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9956, Requested 363. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2ef8cdf3-89dd-4a1c-83f9-d6bc1e6837b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 390. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 390. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 313884b4-3281-4593-9af9-75e645baeb29: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9906, Requested 417. Please try again in 1.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9906, Requested 417. Please try again in 1.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d652e7c-88f2-4ffc-970d-4904c645ae5b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9907, Requested 412. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9907, Requested 412. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17985181-cb23-4706-83e6-28990c59024f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9860, Requested 458. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9860, Requested 458. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 643b8b44-28b7-4d67-aabb-4a857756c3dd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9981, Requested 393. Please try again in 2.244s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9981, Requested 393. Please try again in 2.244s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3c8161e2-dcfb-4500-93f5-74d0a7507906: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9941, Requested 355. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9941, Requested 355. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 041acdd3-974c-4476-9696-cde7e10fb2e9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 370. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 370. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17985181-cb23-4706-83e6-28990c59024f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9921, Requested 375. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9921, Requested 375. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cda8e74c-bdba-48de-bc56-4606d00c6c38: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9912, Requested 460. Please try again in 2.232s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9912, Requested 460. Please try again in 2.232s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b2a5254d-2532-47ef-8491-8911a8d0dc14: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9883, Requested 468. Please try again in 2.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9883, Requested 468. Please try again in 2.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 88c9d767-480b-471e-9675-c18afd412956: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9676, Requested 385. Please try again in 366ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9676, Requested 385. Please try again in 366ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17e33a3a-50c3-496f-a26c-2190cf5e8bde: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9658, Requested 403. Please try again in 366ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9658, Requested 403. Please try again in 366ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d652e7c-88f2-4ffc-970d-4904c645ae5b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9969, Requested 426. Please try again in 2.37s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9969, Requested 426. Please try again in 2.37s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0c7a5850-52bc-4b25-9b3e-aa662c223bad: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9955, Requested 441. Please try again in 2.376s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9955, Requested 441. Please try again in 2.376s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17985181-cb23-4706-83e6-28990c59024f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 396. Please try again in 1.95s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 396. Please try again in 1.95s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 691db2a2-3053-4cb8-a29a-ef2bbbb3ef93: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9839, Requested 555. Please try again in 2.364s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9839, Requested 555. Please try again in 2.364s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 33a3c66e-0be5-49a9-bce5-ae1684e4b880: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9981, Requested 340. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9981, Requested 340. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b2a5254d-2532-47ef-8491-8911a8d0dc14: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9905, Requested 415. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9905, Requested 415. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cda8e74c-bdba-48de-bc56-4606d00c6c38: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9834, Requested 484. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9834, Requested 484. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17e33a3a-50c3-496f-a26c-2190cf5e8bde: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9961, Requested 347. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9961, Requested 347. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17985181-cb23-4706-83e6-28990c59024f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9939, Requested 373. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9939, Requested 373. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0c7a5850-52bc-4b25-9b3e-aa662c223bad: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9941, Requested 371. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9941, Requested 371. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 691db2a2-3053-4cb8-a29a-ef2bbbb3ef93: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9947, Requested 363. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9947, Requested 363. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 33a3c66e-0be5-49a9-bce5-ae1684e4b880: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9981, Requested 348. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9981, Requested 348. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b2a5254d-2532-47ef-8491-8911a8d0dc14: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9922, Requested 407. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9922, Requested 407. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0c7a5850-52bc-4b25-9b3e-aa662c223bad: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9815, Requested 380. Please try again in 1.17s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9815, Requested 380. Please try again in 1.17s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17985181-cb23-4706-83e6-28990c59024f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9784, Requested 411. Please try again in 1.17s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9784, Requested 411. Please try again in 1.17s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 691db2a2-3053-4cb8-a29a-ef2bbbb3ef93: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9689, Requested 494. Please try again in 1.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9689, Requested 494. Please try again in 1.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b2a5254d-2532-47ef-8491-8911a8d0dc14: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9906, Requested 412. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9906, Requested 412. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0c7a5850-52bc-4b25-9b3e-aa662c223bad: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9901, Requested 368. Please try again in 1.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9901, Requested 368. Please try again in 1.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 691db2a2-3053-4cb8-a29a-ef2bbbb3ef93: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9874, Requested 360. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9874, Requested 360. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b2a5254d-2532-47ef-8491-8911a8d0dc14: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9811, Requested 422. Please try again in 1.398s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9811, Requested 422. Please try again in 1.398s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 88c9d767-480b-471e-9675-c18afd412956: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9734, Requested 323. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9734, Requested 323. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17e33a3a-50c3-496f-a26c-2190cf5e8bde: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9969, Requested 344. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9969, Requested 344. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0c7a5850-52bc-4b25-9b3e-aa662c223bad: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 395. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 395. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 691db2a2-3053-4cb8-a29a-ef2bbbb3ef93: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9808, Requested 509. Please try again in 1.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9808, Requested 509. Please try again in 1.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "pdr_rag_results = evaluate(\n",
        "    parent_document_retriever_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Parent Document Retrieval RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaNk6o7_BqX8"
      },
      "source": [
        "### Task 2: Ensemble Retrieval\n",
        "\n",
        "Next let's look at ensemble retrieval!\n",
        "\n",
        "You can read more about this [here](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble)!\n",
        "\n",
        "The basic idea is as follows:\n",
        "\n",
        "1. Obtain User Question\n",
        "2. Hit the Retriever Pair\n",
        "    - Retrieve Documents with BM25 Sparse Vector Retrieval\n",
        "    - Retrieve Documents with Dense Vector Retrieval Method\n",
        "3. Collect and \"fuse\" the retrieved docs based on their weighting using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm into a single ranked list.\n",
        "4. Use those documents to augment our generation.\n",
        "\n",
        "Ensure your `weights` list - the relative weighting of each retriever - sums to 1!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "zz7dl1GD5-L-"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "Vs8wxT9b5pRA"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=75)\n",
        "split_documents = text_splitter.split_documents(documents)\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(split_documents)\n",
        "bm25_retriever.k = 2\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectorstore = Qdrant.from_documents(split_documents, embedding, location=\":memory:\")\n",
        "qdrant_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[\n",
        "        bm25_retriever,\n",
        "        qdrant_retriever\n",
        "    ],\n",
        "    weights=[\n",
        "        0.5,\n",
        "        0.5\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "cv69YDpF6PrJ"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6lSszzrf6UmP",
        "outputId": "ea13ffbc-df0f-4191-f873-6c2f0405d874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in LLM (Large Language Models) application development. It involves connecting LLMs to external data sources to address the lack of recent or private information in the models.'"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "e99b5317081e42c199a297f3a483bbf5",
            "42bfd25ca7a14d629e4070b093a197d5",
            "fa9f9fc79b344271adc94acc487aef59",
            "a603cf6ec7d544a1a951fb8defccb976",
            "7448f6f5d9dc431fba689d8821285ca3",
            "1d0589c63fd1461b93b403c594b6ccfa",
            "5e500672dafc4529806e4e5f72fe97fa",
            "ece363bdc8b54631beba2628e3175e0b",
            "46a7a454e9bb486588ac33156abddecc",
            "f79f87ac933b45fa9e2c17871d4f2e44",
            "a888bb92387549fea8843f20e3b4c50f"
          ]
        },
        "id": "GVBY5lhm4KG7",
        "outputId": "d48d2604-1ac3-4b8a-c4aa-93214b4c0e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Hybrid Retrieval RAG Evaluation-2b534fa8' at:\n",
            "https://smith.langchain.com/o/3e2d3fa4-8d65-5328-ab1f-a089d874c8f0/datasets/9eff5d3e-6c80-4567-811b-bd50d532ff2d/compare?selectedSessions=d2122664-d804-41f2-9f93-1a4aed9b919e\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0e5c573777c4f8ca3095931e3929f9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9a7d4b46-b6e1-4899-9da6-bd4b79ef8e34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9849, Requested 350. Please try again in 1.194s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9849, Requested 350. Please try again in 1.194s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7815c915-5cda-497e-8103-e493fb29333b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9739, Requested 361. Please try again in 600ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9739, Requested 361. Please try again in 600ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cee45ca4-309f-4957-85f2-7194f2c8af96: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9682, Requested 391. Please try again in 438ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9682, Requested 391. Please try again in 438ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8db4733e-8e65-46d6-a133-5e70c8f99262: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9971, Requested 323. Please try again in 1.764s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9971, Requested 323. Please try again in 1.764s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3aa8529b-5821-4568-8dd0-6319eabcaac5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9966, Requested 332. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9966, Requested 332. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 062570d3-eb88-4f5d-92f3-30ddf5bb5a72: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9956, Requested 341. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9956, Requested 341. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 181e5a23-a3f8-4e54-b675-431460a2898c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9951, Requested 340. Please try again in 1.746s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9951, Requested 340. Please try again in 1.746s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 23bb32f4-748d-411b-93d5-92ac41286311: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9947, Requested 349. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9947, Requested 349. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 34e6bf5a-0279-44a8-9b20-abdb131eb7d1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 374. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9925, Requested 374. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b72ea55a-cca4-4d1d-919d-eb620cf2ae86: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9913, Requested 382. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9913, Requested 382. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4517fcca-67c0-490d-8c45-a2014b259472: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9955, Requested 345. Please try again in 1.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9955, Requested 345. Please try again in 1.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a1660744-b467-42c7-ac98-f3213f5c076b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9943, Requested 355. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9943, Requested 355. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9a7d4b46-b6e1-4899-9da6-bd4b79ef8e34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9952, Requested 347. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9952, Requested 347. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7fc2f7ca-cea5-4716-acc4-e5beebd053d5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9932, Requested 367. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9932, Requested 367. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36a99899-7d80-4e2a-868a-97daf5ce9c0b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9930, Requested 370. Please try again in 1.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9930, Requested 370. Please try again in 1.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 181e5a23-a3f8-4e54-b675-431460a2898c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9979, Requested 329. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9979, Requested 329. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7815c915-5cda-497e-8103-e493fb29333b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9947, Requested 356. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9947, Requested 356. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 23bb32f4-748d-411b-93d5-92ac41286311: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9957, Requested 345. Please try again in 1.812s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9957, Requested 345. Please try again in 1.812s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 062570d3-eb88-4f5d-92f3-30ddf5bb5a72: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9977, Requested 332. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9977, Requested 332. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cee45ca4-309f-4957-85f2-7194f2c8af96: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9927, Requested 382. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9927, Requested 382. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b72ea55a-cca4-4d1d-919d-eb620cf2ae86: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9922, Requested 389. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9922, Requested 389. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 34e6bf5a-0279-44a8-9b20-abdb131eb7d1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9912, Requested 395. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9912, Requested 395. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a1660744-b467-42c7-ac98-f3213f5c076b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9680, Requested 345. Please try again in 150ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9680, Requested 345. Please try again in 150ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9a7d4b46-b6e1-4899-9da6-bd4b79ef8e34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9679, Requested 347. Please try again in 156ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9679, Requested 347. Please try again in 156ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36a99899-7d80-4e2a-868a-97daf5ce9c0b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9979, Requested 366. Please try again in 2.07s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9979, Requested 366. Please try again in 2.07s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7fc2f7ca-cea5-4716-acc4-e5beebd053d5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9671, Requested 355. Please try again in 156ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9671, Requested 355. Please try again in 156ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 062570d3-eb88-4f5d-92f3-30ddf5bb5a72: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9970, Requested 339. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9970, Requested 339. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4517fcca-67c0-490d-8c45-a2014b259472: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9678, Requested 346. Please try again in 144ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9678, Requested 346. Please try again in 144ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 181e5a23-a3f8-4e54-b675-431460a2898c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9970, Requested 338. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9970, Requested 338. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 23bb32f4-748d-411b-93d5-92ac41286311: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9958, Requested 346. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9958, Requested 346. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7815c915-5cda-497e-8103-e493fb29333b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9949, Requested 358. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9949, Requested 358. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b72ea55a-cca4-4d1d-919d-eb620cf2ae86: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9970, Requested 379. Please try again in 2.094s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9970, Requested 379. Please try again in 2.094s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cee45ca4-309f-4957-85f2-7194f2c8af96: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9962, Requested 388. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9962, Requested 388. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4517fcca-67c0-490d-8c45-a2014b259472: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9956, Requested 342. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9956, Requested 342. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 34e6bf5a-0279-44a8-9b20-abdb131eb7d1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9971, Requested 371. Please try again in 2.052s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9971, Requested 371. Please try again in 2.052s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 181e5a23-a3f8-4e54-b675-431460a2898c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9955, Requested 344. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9955, Requested 344. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a1660744-b467-42c7-ac98-f3213f5c076b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9944, Requested 352. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9944, Requested 352. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 062570d3-eb88-4f5d-92f3-30ddf5bb5a72: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9950, Requested 347. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9950, Requested 347. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 23bb32f4-748d-411b-93d5-92ac41286311: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9937, Requested 361. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9937, Requested 361. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9a7d4b46-b6e1-4899-9da6-bd4b79ef8e34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9934, Requested 362. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9934, Requested 362. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36a99899-7d80-4e2a-868a-97daf5ce9c0b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9950, Requested 367. Please try again in 1.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9950, Requested 367. Please try again in 1.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7815c915-5cda-497e-8103-e493fb29333b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9944, Requested 372. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9944, Requested 372. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7fc2f7ca-cea5-4716-acc4-e5beebd053d5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9952, Requested 364. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9952, Requested 364. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4517fcca-67c0-490d-8c45-a2014b259472: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9985, Requested 361. Please try again in 2.076s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9985, Requested 361. Please try again in 2.076s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7e7d3b89-2f1d-43e5-898c-9e5af91d48ae: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9965, Requested 384. Please try again in 2.094s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9965, Requested 384. Please try again in 2.094s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cee45ca4-309f-4957-85f2-7194f2c8af96: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9951, Requested 398. Please try again in 2.094s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9951, Requested 398. Please try again in 2.094s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b72ea55a-cca4-4d1d-919d-eb620cf2ae86: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9942, Requested 404. Please try again in 2.076s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9942, Requested 404. Please try again in 2.076s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e690b530-b5e0-4dfb-bdd3-2c079f22238d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9948, Requested 402. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9948, Requested 402. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 34e6bf5a-0279-44a8-9b20-abdb131eb7d1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9937, Requested 410. Please try again in 2.082s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9937, Requested 410. Please try again in 2.082s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8b9b257b-98fb-42a1-bca0-2339a6ab0a97: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9906, Requested 442. Please try again in 2.088s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9906, Requested 442. Please try again in 2.088s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8f020f86-c77d-4066-9da0-7705d4a7049e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9849, Requested 499. Please try again in 2.088s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9849, Requested 499. Please try again in 2.088s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7fc2f7ca-cea5-4716-acc4-e5beebd053d5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9926, Requested 371. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9926, Requested 371. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 80ee97a7-7ac4-4ef2-a626-f2f92c8ab534: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9991, Requested 392. Please try again in 2.298s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9991, Requested 392. Please try again in 2.298s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36a99899-7d80-4e2a-868a-97daf5ce9c0b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 10000, Requested 381. Please try again in 2.286s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 10000, Requested 381. Please try again in 2.286s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7e7d3b89-2f1d-43e5-898c-9e5af91d48ae: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9741, Requested 334. Please try again in 450ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9741, Requested 334. Please try again in 450ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8db4733e-8e65-46d6-a133-5e70c8f99262: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9747, Requested 327. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9747, Requested 327. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e690b530-b5e0-4dfb-bdd3-2c079f22238d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9725, Requested 346. Please try again in 426ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9725, Requested 346. Please try again in 426ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8b9b257b-98fb-42a1-bca0-2339a6ab0a97: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9702, Requested 372. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9702, Requested 372. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4ec8f90e-9b01-4b14-a70d-d361c5f5311e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9904, Requested 405. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9904, Requested 405. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8f020f86-c77d-4066-9da0-7705d4a7049e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9878, Requested 431. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9878, Requested 431. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d956875-6384-4322-b895-e2e1c023bd05: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9884, Requested 557. Please try again in 2.646s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9884, Requested 557. Please try again in 2.646s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 80ee97a7-7ac4-4ef2-a626-f2f92c8ab534: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9784, Requested 334. Please try again in 708ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9784, Requested 334. Please try again in 708ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3aa8529b-5821-4568-8dd0-6319eabcaac5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9781, Requested 336. Please try again in 702ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9781, Requested 336. Please try again in 702ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4ec8f90e-9b01-4b14-a70d-d361c5f5311e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9967, Requested 345. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9967, Requested 345. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e690b530-b5e0-4dfb-bdd3-2c079f22238d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9973, Requested 341. Please try again in 1.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9973, Requested 341. Please try again in 1.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8b9b257b-98fb-42a1-bca0-2339a6ab0a97: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 381. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9929, Requested 381. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8f020f86-c77d-4066-9da0-7705d4a7049e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9870, Requested 437. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9870, Requested 437. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 80ee97a7-7ac4-4ef2-a626-f2f92c8ab534: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9726, Requested 330. Please try again in 336ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9726, Requested 330. Please try again in 336ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d956875-6384-4322-b895-e2e1c023bd05: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9692, Requested 365. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9692, Requested 365. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4ec8f90e-9b01-4b14-a70d-d361c5f5311e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9978, Requested 343. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9978, Requested 343. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e690b530-b5e0-4dfb-bdd3-2c079f22238d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9973, Requested 343. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9973, Requested 343. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8b9b257b-98fb-42a1-bca0-2339a6ab0a97: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9950, Requested 369. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9950, Requested 369. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8f020f86-c77d-4066-9da0-7705d4a7049e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9890, Requested 428. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9890, Requested 428. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e690b530-b5e0-4dfb-bdd3-2c079f22238d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9975, Requested 356. Please try again in 1.985s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9975, Requested 356. Please try again in 1.985s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8b9b257b-98fb-42a1-bca0-2339a6ab0a97: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9936, Requested 396. Please try again in 1.992s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9936, Requested 396. Please try again in 1.992s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d956875-6384-4322-b895-e2e1c023bd05: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9831, Requested 496. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9831, Requested 496. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8f020f86-c77d-4066-9da0-7705d4a7049e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9961, Requested 452. Please try again in 2.478s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9961, Requested 452. Please try again in 2.478s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d956875-6384-4322-b895-e2e1c023bd05: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9726, Requested 362. Please try again in 528ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9726, Requested 362. Please try again in 528ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d956875-6384-4322-b895-e2e1c023bd05: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9779, Requested 511. Please try again in 1.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-AdhcHEVRXYxcJVZh94lEyz9i on tokens per min (TPM): Limit 10000, Used 9779, Requested 511. Please try again in 1.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "pdr_rag_results = evaluate(\n",
        "    ensemble_retriever_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Hybrid Retrieval RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "#### â“Question #1:\n",
        "\n",
        "What conclusions can you draw about the above results?\n",
        "\n",
        "Describe in your own words what the metrics are expressing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "087e4c759a35424385156c76e3780144": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef2e833ac73548ee837f906d4b004a3d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2c984bd9861c4ec0b27f0c3a2020dede",
            "value": ""
          }
        },
        "13c75ca5ee9d42eb8476a307f30b7528": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "142e5756a6c840aeb25b733ece015ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4777a17d7d9c444a8d44e070faef0ad0",
              "IPY_MODEL_dad645f3259c41b5a50e0e94906ca3e3",
              "IPY_MODEL_c67428191d074e4d80cb0f28fb65cd41"
            ],
            "layout": "IPY_MODEL_8db8082c5ab741b5b4fd78027e65135d"
          }
        },
        "18ffc7ae91d1408c8747f8d0a972f3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9927b2052be4016b0feb88ab583d32e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fbddf6663954ff48ee0cb0b4a202acb",
            "value": 1
          }
        },
        "1d0589c63fd1461b93b403c594b6ccfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e2bff47a084ed6bc7d96ac09af9ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2ab8452345f24b46b172515ddc77fdb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c984bd9861c4ec0b27f0c3a2020dede": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42bfd25ca7a14d629e4070b093a197d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d0589c63fd1461b93b403c594b6ccfa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5e500672dafc4529806e4e5f72fe97fa",
            "value": ""
          }
        },
        "46a7a454e9bb486588ac33156abddecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4777a17d7d9c444a8d44e070faef0ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab8452345f24b46b172515ddc77fdb0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d5ad942ac5ed41c6a581b4022f177114",
            "value": ""
          }
        },
        "5e500672dafc4529806e4e5f72fe97fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65d1b263094a4c6abdaf8688285c41cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_087e4c759a35424385156c76e3780144",
              "IPY_MODEL_18ffc7ae91d1408c8747f8d0a972f3a8",
              "IPY_MODEL_8cbac09f7ba540bd8f79dde47e49fe5f"
            ],
            "layout": "IPY_MODEL_b5e2be01c1834c78b7ce0cbe1f7a9d09"
          }
        },
        "6fbddf6663954ff48ee0cb0b4a202acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7448f6f5d9dc431fba689d8821285ca3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbac09f7ba540bd8f79dde47e49fe5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6035d68dd9e4c6598ac5eae2d0deac1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b6b1f2be05a34e118dc55869149b60ac",
            "value": "â€‡12/?â€‡[01:07&lt;00:00,â€‡â€‡2.29s/it]"
          }
        },
        "8db8082c5ab741b5b4fd78027e65135d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a603cf6ec7d544a1a951fb8defccb976": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f79f87ac933b45fa9e2c17871d4f2e44",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a888bb92387549fea8843f20e3b4c50f",
            "value": "â€‡22/?â€‡[01:19&lt;00:00,â€‡â€‡2.61s/it]"
          }
        },
        "a888bb92387549fea8843f20e3b4c50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5e2be01c1834c78b7ce0cbe1f7a9d09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6b1f2be05a34e118dc55869149b60ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c67428191d074e4d80cb0f28fb65cd41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13c75ca5ee9d42eb8476a307f30b7528",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e8143f8ceee6468c98433734ff59b835",
            "value": "â€‡22/?â€‡[01:16&lt;00:00,â€‡â€‡2.24s/it]"
          }
        },
        "c9927b2052be4016b0feb88ab583d32e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d097ee02256d40d5b395d37cd0face05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5ad942ac5ed41c6a581b4022f177114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dad645f3259c41b5a50e0e94906ca3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e2bff47a084ed6bc7d96ac09af9ebc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d097ee02256d40d5b395d37cd0face05",
            "value": 1
          }
        },
        "e6035d68dd9e4c6598ac5eae2d0deac1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8143f8ceee6468c98433734ff59b835": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e99b5317081e42c199a297f3a483bbf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42bfd25ca7a14d629e4070b093a197d5",
              "IPY_MODEL_fa9f9fc79b344271adc94acc487aef59",
              "IPY_MODEL_a603cf6ec7d544a1a951fb8defccb976"
            ],
            "layout": "IPY_MODEL_7448f6f5d9dc431fba689d8821285ca3"
          }
        },
        "ece363bdc8b54631beba2628e3175e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ef2e833ac73548ee837f906d4b004a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f79f87ac933b45fa9e2c17871d4f2e44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9f9fc79b344271adc94acc487aef59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece363bdc8b54631beba2628e3175e0b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46a7a454e9bb486588ac33156abddecc",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
